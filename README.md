# PyTorch Transformer Implementation: Attention Is All You Need

This repository provides a comprehensive PyTorch implementation of the groundbreaking Transformer architecture introduced in the paper [**"Attention Is All You Need"**](https://arxiv.org/abs/1706.03762). The implementation includes the full encoder-decoder structure with multi-head attention mechanisms, positional encoding, and residual connections.

## ğŸš€ Features
- Complete Transformer model implementation
- Training pipeline with configurable hyperparameters
- Customizable dataset handling
- PyTorch-based implementation

## âš™ï¸ Requirements
| Package       | Minimum Version |
|---------------|-----------------|
| Python        | 3.96            |
| PyTorch       | 1.12.0          |
| TorchText     | 0.13.0          |
| NumPy         | 1.26.4          |
| [UV](https://github.com/astral-sh/uv) | Latest          |

## ğŸ“‚ Project Structure
```bash
.
â”œâ”€â”€ config/           # Configuration files (hyperparameters, paths)
â”‚   â””â”€â”€ config.yaml   # The yaml file for configuration
â”œâ”€â”€ datasets/         # Dataset handling
â”‚   â”œâ”€â”€ dataset       # The txt file for dataset
â”‚   â”œâ”€â”€ dataset.py    # Dataloader class for training or interence
â”‚   â””â”€â”€ util.py       # Read data from txt file
â”œâ”€â”€ models/           # Transformer implementation
â”‚   â”œâ”€â”€ embedding/    # Tolen embedding and positional embedding
â”‚   â”œâ”€â”€ layers/       # Encoder and Decoder
â”‚   â”œâ”€â”€ model/        # Model architecture
â”‚   â””â”€â”€ utils/        # Utils for model, such as masking
â”œâ”€â”€ train.py          # Main training script
â”œâ”€â”€ infer.py          # Main interence script
â”œâ”€â”€ requirements.txt  # This dependencies
â””â”€â”€ README.md         # This documentation
```

## ğŸš€ Quick Start

### Clone the repository
```bash
git clone git@github.com:fanfan-yu/transformer.git
cd transformer
```

### Install UV package manager
```bash
pip install uv
```

### Download the dependencies
```bash
# Initialize environment and install dependencies
uv init
uv sync
```

### Run the Project
```bash
# Start training the Transformer model
uv run train.py
```

## ğŸ”§ Configuration
The `config/config.yaml` file contains all configurable parameters:
```yaml
path:
  sentences_path: ./datasets/dataset/sentences.txt
  src_vocab_path: ./datasets/dataset/src_vocab.txt
  tgt_vocab_path: ./datasets/dataset/tgt_vocab.txt
# model parameters
model:
  d_model: 512
  n_head: 8
  d_key: 64
  d_value: 64
  d_feedforward: 2048
  max_len: 5000
  num_encoder_layers: 6
  num_decoder_layers: 6
# train parameters
train:
  batch_size: 2
  epoch: 20
  learning_rate: 0.001
  dropout: 0.1
```
## ğŸ“Š Todo List
- [ ] Implement inference
- [ ] Add support for external datasets (WMT, WikiText)
- [ ] Create Jupyter Notebook tutorials for beginners
- [ ] Optimize training for GPU environments

## ğŸ¤ Contributing
Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create your feature branch (git checkout -b feat/your-feature)
3. Commit your changes (git commit -am 'Add some feature')
4. Push to the branch (git push origin feature/your-feature)
5. Open a pull request

## ğŸ“œ License
This project is licensed under the MIT License - see the LICENSE file for details.
